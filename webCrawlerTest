from bs4 import BeautifulSoup
from urllib.parse import urljoin

import csv
import re
import requests

csv_headers=["Name","URL","Contact Name","Contact Title","Phone Number","Additional Phone Number","Address","Contact Email","Brand","Location","Primary SIC Category"]

def extract_contacts(soup):
    phone_numbers_pattern = re.compile(r'\(?\b\d{3}[-.\s]?\)?\d{3}[-.\s]?\d{4}\b')
    email_patterns = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')

    # Find relevant HTML tags containing contact information
    phone_numbers_tags = soup.find_all(string=phone_numbers_pattern)
    email_tags = soup.find_all(string=email_patterns)

    # Use sets to remove duplicates
    unique_phone_numbers = set()
    unique_email_addresses = set()

    for tag in phone_numbers_tags:
        unique_phone_numbers.update(phone_numbers_pattern.findall(tag))

    for tag in email_tags:
        unique_email_addresses.update(email_patterns.findall(tag))

    # Extract information and fill missing values with empty strings
    extracted_info = {
        "Phone Number": "; ".join(map(str.strip, unique_phone_numbers)),
        "Contact Email": "; ".join(map(str.strip, unique_email_addresses))
    }

    for header in csv_headers:
        if header not in extracted_info:
            extracted_info[header] = ""

    return extracted_info



def simple_web_crawler(url, max_depth=3, current_depth=1, visited_urls=None, output_file="output.csv"):
  if visited_urls is None:
    visited_urls = set()
  try:
    response = requests.get(url)
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extract contact information
        contact_info = extract_contacts(soup)
        with open(output_file, mode='a', newline='', encoding='utf-8') as file:
                writer = csv.DictWriter(file, fieldnames=csv_headers)
                if file.tell() == 0:  # Write headers only if the file is empty
                    writer.writeheader()
                writer.writerow(contact_info)

        links = soup.find_all('a')
        if current_depth < max_depth:
          for link in links:
            href = link.get('href')
            if href and not href.startswith('#'):  # Ignore internal page links
              absolute_url = urljoin(url, href)
              if absolute_url not in visited_urls:
                visited_urls.add(absolute_url)
                simple_web_crawler(absolute_url, max_depth, current_depth + 1, visited_urls, output_file=output_file)
    else:
      print(f"Failed to retrieve content from {url}. Status code: {response.status_code}")
  except Exception as e:
    print(f"An error occurred: {str(e)}")

url_to_crawl = "https://ccafla.com/"
output_file_path = "output.csv"
simple_web_crawler(url_to_crawl, output_file=output_file_path)
